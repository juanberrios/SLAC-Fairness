---
title: Data analysis
author: Juan Berr√≠os, Dan Villareal
date: "03/16/2025"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    code_folding: show
    includes:
      in_header: "_includes/head-custom.html"
params:
  extract_metrics: TRUE 
  extract_only_metrics: FALSE 
---

```{r setup, include=FALSE}
##Start timer
timing <- list()
timing$start <- proc.time()

##Handle parameters
extract_only_metrics <- ifelse(params$extract_metrics, params$extract_only_metrics, FALSE)

##knitr settings
knitr::opts_chunk$set(eval=!extract_only_metrics, echo=TRUE, include=TRUE, 
                      comment=NA, results="hold")

##Packages
library(tidyverse)    # tidyverse 'dialect' of R
library(magrittr)     # nicer aliases (see ?add)
library(knitr)        # combine text, code, and code output
library(ggrepel)      # repel plot labels
library(rPref)        # Pareto analysis
library(benchmarkme)  # get_cpu(), get_ram()

##UMS functions

source("R-Scripts/UMS-Utils.R", keep.source=TRUE)
```

# Introduction {#intro}

- This notebook documents the implementation and analysis of unfairness mitigation strategies (UMSs) on the random-forest classifier for automated coding of non-prevocalic /r/ that was trained in a prior notebook. We attempt to produce a fair auto-coder that does not suffer from fairness issues. The code in this notebook is based on the following [tutorial](https://github.com/djvill/SLAC-Fairness/tree/main/Analysis-Walkthrough.Rmd) that documents the use of UMSs on [a different classifier](https://nzilbb.github.io/How-to-Train-Your-Classifier/How_to_Train_Your_Classifier.html).


```{r}
##Read in data

##Initial classifier
fstInit <- readRDS("../Overlearning-Race-Classifier/Processed Data for Classifier/code/Model Status/fstInit.Rds")

##Training data

trainingData <- readRDS("../Overlearning-Race-Classifier/Processed Data for Classifier/code/Model Status/trainingData.Rds")

##Pre-processing grouping columns as factor to prevent contrast errors

trainingData <- trainingData %>% 
  mutate(race = as.factor(race)) %>%
  mutate(rpresent = as.factor(rpresent)) 

##Save data for use by R scripts

saveRDS(trainingData, file = "Input-Data/trainingData.Rds")
```

# Mitigating SLAC unfairness

- In this section, we attempt to produce an auto-coder that does not suffer from the fairness issues previously identified. To do this, we run and analyze additional auto-coders under different unfairness mitigation strategies (UMSs). Read more about UMSs in 
the paper [Sociolinguistic auto-coding has fairness problems too: Measuring and mitigating overlearning bias (Linguistics Vanguard, 2024)](https://www.degruyter.com/document/doi/10.1515/lingvan-2022-0114/html) and the [sample Analysis Walkthrough](https://djvill.github.io/SLAC-Fairness/Analysis-Walkthrough#2_RQ2:_Assessing_fairness_for_SLAC). 

## Baseline {#baseline}

- We'll run first run an un-optimized baseline auto-coder for proper comparisons.

### Run auto-coder {#baseline-run}

- To dot this, we load Bash, navigate to the `Shell-Scripts/` directory, and run the following:

```{bash, eval=FALSE}
bash Baseline.sh &> ../Outputs/Shell-Scripts/Run-Baseline.out
```

- Once that script is done running, a new auto-coder file is generated: `Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.0.Rds`.

### Extract fairness and performance metrics {#baseline-metrics}

- We'll now extract fairness/performance metrics from this auto-coder (using `cls_summary()`) and save metrics to `Outputs/Performance/`. To extract and save fairness/performance metrics from the baseline auto-coder:

```{r, eval=params$extract_metrics}
##Get list of UMS descriptions
umsList <- read.csv("Input-Data/UMS-List.txt", sep="\t")

##Read auto-coder file
file_baseline <- "Run-UMS_UMS0.0.Rds"
cls_baseline <- readRDS(paste0("Outputs/Diagnostic-Files/Temp-Autocoders/", 
                               file_baseline))

##Extract performance
cls_baseline %>% 
  cls_summary() %>%
  ##Add name and long description
  mutate(Classifier = str_remove_all(file_baseline, ".+_|\\.Rds"),
         .before=1) %>% 
  left_join(umsList %>% 
              mutate(Classifier = paste0("UMS", UMS)) %>% 
              select(-UMS),
            by="Classifier") %>% 
  ##Save data
  write_csv("Outputs/Performance/Perf_Baseline.csv")
```

- We won't analyze baseline fairness here because its whole purpose is to compare UMSs against it (with neither the baseline nor UMS auto-coders optimized for performance). However, it's worth noting that there are small differences in fairness/performance between this un-optimized baseline and the previously trained auto-coder:

```{r}
##Read baseline performance
perf_baseline <- read_csv("Outputs/Performance/Perf_Baseline.csv")

##Combine Initial classifier & Baseline metrics into a single dataframe
list(Initial = cls_summary(fstInit),
     Baseline = perf_baseline %>% 
       select(-c(Classifier, Description))) %>% 
  ##One dataframe with just the necessary columns
  map_dfr(select, Acc, Acc_Diff, matches("ClassAcc_(Present|Absent)$"),
          matches("ClassAcc_(Present|Absent)_Diff"),
          .id="Version") %>%
  ##Initial/Baseline in separate columns, one row per metric * type
  pivot_longer(contains("Acc"), names_to="Metric") %>%
  mutate(Type = if_else(str_detect(Metric, "Diff"), "Fairness", "Performance"),
         Metric = fct_inorder(if_else(str_detect(Metric, "Absent|Present"),
                                      paste(str_extract(Metric, "Absent|Present"), "class accuracy"),
                                      "Overall accuracy"))) %>%
  pivot_wider(names_from=Version) %>%
  ##Put rows in nicer order
  arrange(Metric, desc(Type))
```


## UMS round 1

- The auto-coders in UMS round 1 include downsampling and valid predictor selection UMSs.

### Run auto-coders and extract metrics {#ums1-run}

- We'll do this again using Bash, navigating to `Shell-Scripts/`, and running the following:

```{bash, eval=FALSE}
##Run directly
bash UMS-Round1.sh &> ../Outputs/Shell-Scripts/UMS-Round1.out
```

- Once that script is done running, there should files corresponding to each new classifier/UMS in `Outputs/Diagnostic-Files/Temp-Autocoders/`. We'll now extract and save fairness/performance metrics from the round 1 auto-coders:

```{r, eval=params$extract_metrics}
##Get auto-coder filenames (exclude UMS 0.x precursor auto-coders)
files_round1 <- list.files("Outputs/Diagnostic-Files/Temp-Autocoders/", 
                           "Run-UMS_UMS[1-3]", full.names=TRUE)

##Read auto-coder files
cls_round1 <-
  files_round1 %>% 
  ##Better names
  set_names(str_remove_all(files_round1, ".+_|\\.Rds")) %>% 
  map(readRDS)

##Extract performance
cls_round1 %>% 
  map_dfr(cls_summary, .id="Classifier") %>% 
  ##Add long description
  left_join(umsList %>% mutate(Classifier = paste0("UMS", UMS)) %>% select(-UMS),
            by="Classifier") %>% 
  ##Save data
  write_csv("Outputs/Performance/Perf_UMS-Round1.csv")
```

- We'll also extract and save variable importance data from particular auto-coders.
Several UMSs are "valid predictor selection" strategies: they remove acoustic measures that could inadvertently signal speaker race. To determine which measures could inadvertently signal speaker race, we run an auto-coder predicting _speaker race_ rather than rhoticity and discard the measures that were "too helpful" in predicting gender. The following code pulls variable importance from these "precursor" auto-coders.

```{r, eval=params$extract_metrics}
##UMS 0.1.1
readRDS("Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.1.1.Rds") %>% 
  pluck("finalModel", "variable.importance") %>% 
  {tibble(Measure=names(.), Importance=.)} %>% 
  write.csv("Outputs/Other/Var-Imp_UMS0.1.1.csv", row.names=FALSE)

##UMS 0.2
readRDS("Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.2.Rds") %>% 
  map_dfr(~ .x %>% 
            pluck("finalModel", "variable.importance") %>% 
            {tibble(Measure=names(.), value=.)},
          .id="name") %>% 
  pivot_wider(names_prefix="Importance_") %>% 
  write.csv("Outputs/Other/Var-Imp_UMS0.2.csv", row.names=FALSE)
```

- This step isn't strictly necessary for the R code in this document, but it allows us to run [`umsData()`](index#r-scripts) for all UMSs without needing to be on the computer where the auto-coders are saved (if it's different).

### Analyze fairness and performance

- We'll start reading fairness/performance data for the round 1 auto-coders, and add baseline data:

```{r}
##Read
perf_baseline <- read_csv("Outputs/Performance/Perf_Baseline.csv")
perf_round1 <- rbind(perf_baseline,
                     read_csv("Outputs/Performance/Perf_UMS-Round1.csv"))

##Decode first digit of UMS code
categories <- c("Baseline", "Downsampling", "Valid pred selection", 
                "Normalization", "Combination") %>% 
  set_names(0:4)

##Shape performance dataframe for plotting: Fairness/Performance in separate
##  columns, one row per UMS * metric, Category factor, shorter Classifier label
perfPlot_round1 <- perf_round1 %>%
  select(Classifier,
         Acc, Acc_Diff, matches("ClassAcc_(Present|Absent)$"),
         matches("ClassAcc_(Present|Absent)_Diff")) %>%
  ##Fairness/Performance in separate columns, one row per UMS * metric
  pivot_longer(contains("Acc")) %>%
  mutate(Metric = fct_inorder(if_else(str_detect(name, "Absent|Present"),
                                      paste(str_extract(name, "Absent|Present"), "class accuracy"),
                                      "Overall accuracy")),
         name = if_else(str_detect(name, "Diff"), "Fairness", "Performance")) %>%
  pivot_wider() %>%
  ##Add Category column, shorter Classifier label
  mutate(Category = recode_factor(str_extract(Classifier, "\\d"), !!!categories),
         across(Classifier, ~ str_remove(.x, "UMS")))
```


```{r}
##Plot
perfPlot_round1 %>% 
  mutate(across(Fairness, abs)) %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=20) +
  ##Each metric in its own facet
  ##  (N.B. use arg scales="free" to have each facet zoom to fit data)
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw()
```

- To make the baseline stand out, we can plot it with separate aesthetics:

```{r}
perfPlot_round1 %>%
  mutate(across(Fairness, abs)) %>% 
  ##Exclude Baseline from points & labels
  filter(Classifier != "0.0") %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=20) +
  ##Dotted line for baseline
  geom_vline(data=perfPlot_round1 %>% filter(Classifier=="0.0"), 
             aes(xintercept=abs(Fairness)), linetype="dashed") +
  geom_hline(data=perfPlot_round1 %>% filter(Classifier=="0.0"), 
             aes(yintercept=Performance), linetype="dashed") +
  ##Each metric in its own facet
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw()
```

- There are several UMSs that  improve on fairness relative to the baseline, but there isn't a clear cut optimal UMS, i.e., no UMS does well across all metrics. UMS 1.3.1. is better in overall fairness and accuracy and present class fairness and accuracy, but both performance and fairness are poor for absent class accuracy. UMS 1.2 is better in overall fairness and around the same in performance as the baseline in overall accuracy, better for both metrics in absent class accuracy, but worse in both fairness and performance in present class accuracy (although not as markedly). The remaining metrics cluster around the baseline.

# Script meta-info {#script-info}

## R session info

```{r, eval=TRUE}
sessionInfo()
```


## Disk space used

These are only shown if `params$extract_metrics` is `TRUE` (because otherwise it's assumed that you're not working on the same system the auto-coders were run on).

Temporary auto-coders:

```{r, eval=params$extract_metrics}
tmpAuto <- 
  list.files("Outputs/Diagnostic-Files/Temp-Autocoders/", 
             # "^(Run-UMS|(Hyperparam-Tuning|Outlier-Dropping)_UMS0\\.0).*Rds$") %>%
             "^Run-UMS.*Rds$", full.names=TRUE) %>%
  file.info()
# cat("Disk space: ", round(sum(tmpAuto$size)/2^30, 1), " Gb (",
cat("Disk space: ", round(sum(tmpAuto$size)/2^20, 1), " Mb (",
    nrow(tmpAuto), " files)", sep="")
```

Complete repository:

```{r, eval=params$extract_metrics}
if (.Platform$OS.type=="windows") {
  shell("dir /s", intern=TRUE) %>% 
    tail(2) %>% 
    head(1) %>% 
    str_trim() %>% 
    str_squish()
}
if (.Platform$OS.type=="unix") {
  system2("du", "-sh", stdout=TRUE) %>% 
    str_remove("\\s.+") %>% 
    paste0("b")
}
```

## Machine specs

System:

```{r, eval=TRUE}
Sys.info()
```

Processor:

```{r, eval=TRUE}
get_cpu()
```

RAM:

```{r, eval=TRUE}
#get_ram()
```


## Running time

Total running time for R code in this document (with `params$extract_metrics` set to `r params$extract_metrics`), in seconds:

```{r, eval=TRUE}
timing$stop <- proc.time()

timing$stop - timing$start
```

<!-- Footer: Add CSS styling -->

```{css, echo=FALSE}
/* Add scrollbars to long R input/output blocks */
pre {
  max-height: 300px;
  overflow-y: auto;
}

/* Repo navigation block */
div#repo-nav {
  background-color: #ddd;
  width: fit-content;
  padding: 10px 20px;
  border: 1px solid #bbb;
  border-radius: 20px;
}
div#repo-nav * {
  margin: 0px;
  font-style: italic;
}
```

