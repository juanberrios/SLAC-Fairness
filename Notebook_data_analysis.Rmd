---
title: Data analysis
author: Juan Berríos, Dan Villareal
date: "03/31/2025"
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: yes
    df_print: paged
    code_folding: show
    includes:
      in_header: "_includes/head-custom.html"
params:
  extract_metrics: TRUE 
  extract_only_metrics: FALSE 
---

```{r setup, include=FALSE}
##Start timer
timing <- list()
timing$start <- proc.time()

##Handle parameters
extract_only_metrics <- ifelse(params$extract_metrics, params$extract_only_metrics, FALSE)

##knitr settings
knitr::opts_chunk$set(eval=!extract_only_metrics, echo=TRUE, include=TRUE, 
                      comment=NA, results="hold")

##Packages
library(tidyverse)    # tidyverse 'dialect' of R
library(magrittr)     # nicer aliases (see ?add)
library(knitr)        # combine text, code, and code output
library(ggrepel)      # repel plot labels
library(rPref)        # Pareto analysis
library(benchmarkme)  # get_cpu(), get_ram()

##UMS functions

source("R-Scripts/UMS-Utils.R", keep.source=TRUE)
```

# Introduction {#intro}

- This notebook documents the implementation and analysis of unfairness mitigation strategies (UMSs) on the random-forest classifier for automated coding of non-prevocalic /r/ that was trained in a prior notebook. We attempt to produce a fair auto-coder that does not suffer from fairness issues. The code in this notebook is based on the following [tutorial](https://github.com/djvill/SLAC-Fairness/tree/main/Analysis-Walkthrough.Rmd) that documents the use of UMSs on [a different classifier](https://nzilbb.github.io/How-to-Train-Your-Classifier/How_to_Train_Your_Classifier.html).


```{r}
##Read in data

##Initial classifier
fstInit <- readRDS("../Overlearning-Race-Classifier/Processed Data for Classifier/code/Model Status/fstInit.Rds")

##Training data

trainingData <- readRDS("../Overlearning-Race-Classifier/Processed Data for Classifier/code/Model Status/trainingData.Rds")

##Pre-processing grouping columns as factor to prevent contrast errors

trainingData <- trainingData %>% 
  mutate(race = as.factor(race)) %>%
  mutate(rpresent = as.factor(rpresent)) 

##Save data for use by R scripts

saveRDS(trainingData, file = "Input-Data/trainingData.Rds")
```

# Mitigating SLAC unfairness

- In this section, we attempt to produce an auto-coder that does not suffer from the fairness issues previously identified. To do this, we run and analyze additional auto-coders under different unfairness mitigation strategies (UMSs). Read more about UMSs in 
the paper [Sociolinguistic auto-coding has fairness problems too: Measuring and mitigating overlearning bias (Linguistics Vanguard, 2024)](https://www.degruyter.com/document/doi/10.1515/lingvan-2022-0114/html) and the [sample Analysis Walkthrough](https://djvill.github.io/SLAC-Fairness/Analysis-Walkthrough#2_RQ2:_Assessing_fairness_for_SLAC). 

## Baseline {#baseline}

- We'll run first run an un-optimized baseline auto-coder for proper comparisons.

### Run auto-coder {#baseline-run}

- To dot this, we load Bash, navigate to the `Shell-Scripts/` directory, and run the following:

```{bash, eval=FALSE}
bash Baseline.sh &> ../Outputs/Shell-Scripts/Run-Baseline.out
```

- Once that script is done running, a new auto-coder file is generated: `Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.0.Rds`.

### Extract fairness and performance metrics {#baseline-metrics}

- We'll now extract fairness/performance metrics from this auto-coder (using `cls_summary()`) and save metrics to `Outputs/Performance/`. To extract and save fairness/performance metrics from the baseline auto-coder:

```{r, eval=params$extract_metrics}
##Get list of UMS descriptions
umsList <- read.csv("Input-Data/UMS-List.txt", sep="\t")

##Read auto-coder file
file_baseline <- "Run-UMS_UMS0.0.Rds"
cls_baseline <- readRDS(paste0("Outputs/Diagnostic-Files/Temp-Autocoders/", 
                               file_baseline))

##Extract performance
cls_baseline %>% 
  cls_summary() %>%
  ##Add name and long description
  mutate(Classifier = str_remove_all(file_baseline, ".+_|\\.Rds"),
         .before=1) %>% 
  left_join(umsList %>% 
              mutate(Classifier = paste0("UMS", UMS)) %>% 
              select(-UMS),
            by="Classifier") %>% 
  ##Save data
  write_csv("Outputs/Performance/Perf_Baseline.csv")
```

- We won't analyze baseline fairness here because its whole purpose is to compare UMSs against it (with neither the baseline nor UMS auto-coders optimized for performance). However, it's worth noting that there are small differences in fairness/performance between this un-optimized baseline and the previously trained auto-coder:

```{r}
##Read baseline performance
perf_baseline <- read_csv("Outputs/Performance/Perf_Baseline.csv")

##Combine Initial classifier & Baseline metrics into a single dataframe
list(Initial = cls_summary(fstInit),
     Baseline = perf_baseline %>% 
       select(-c(Classifier, Description))) %>% 
  ##One dataframe with just the necessary columns
  map_dfr(select, Acc, Acc_Diff, matches("ClassAcc_(Present|Absent)$"),
          matches("ClassAcc_(Present|Absent)_Diff"),
          .id="Version") %>%
  ##Initial/Baseline in separate columns, one row per metric * type
  pivot_longer(contains("Acc"), names_to="Metric") %>%
  mutate(Type = if_else(str_detect(Metric, "Diff"), "Fairness", "Performance"),
         Metric = fct_inorder(if_else(str_detect(Metric, "Absent|Present"),
                                      paste(str_extract(Metric, "Absent|Present"), "class accuracy"),
                                      "Overall accuracy"))) %>%
  pivot_wider(names_from=Version) %>%
  ##Put rows in nicer order
  arrange(Metric, desc(Type))
```


## UMS round 1

- The auto-coders in UMS round 1 include downsampling and valid predictor selection UMSs.

### Run auto-coders and extract metrics {#ums1-run}

- We'll do this again using Bash, navigating to `Shell-Scripts/`, and running the following:

```{bash, eval=FALSE}
##Run directly
bash UMS-Round1.sh &> ../Outputs/Shell-Scripts/UMS-Round1.out
```

- Once that script is done running, there should files corresponding to each new classifier/UMS in `Outputs/Diagnostic-Files/Temp-Autocoders/`. We'll now extract and save fairness/performance metrics from the round 1 auto-coders:

```{r, eval=params$extract_metrics}
##Get auto-coder filenames (exclude UMS 0.x precursor auto-coders)
files_round1 <- list.files("Outputs/Diagnostic-Files/Temp-Autocoders/", 
                           "Run-UMS_UMS[1-3]", full.names=TRUE)

##Read auto-coder files
cls_round1 <-
  files_round1 %>% 
  ##Better names
  set_names(str_remove_all(files_round1, ".+_|\\.Rds")) %>% 
  map(readRDS)

##Extract performance
cls_round1 %>% 
  map_dfr(cls_summary, .id="Classifier") %>% 
  ##Add long description
  left_join(umsList %>% mutate(Classifier = paste0("UMS", UMS)) %>% select(-UMS),
            by="Classifier") %>% 
  ##Save data
  write_csv("Outputs/Performance/Perf_UMS-Round1.csv")
```

- We'll also extract and save variable importance data from particular auto-coders.
Several UMSs are "valid predictor selection" strategies: they remove acoustic measures that could inadvertently signal speaker race. To determine which measures could inadvertently signal speaker race, we run an auto-coder predicting _speaker race_ rather than rhoticity and discard the measures that were "too helpful" in predicting gender. The following code pulls variable importance from these "precursor" auto-coders.

```{r, eval=params$extract_metrics}
##UMS 0.1.1
readRDS("Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.1.1.Rds") %>% 
  pluck("finalModel", "variable.importance") %>% 
  {tibble(Measure=names(.), Importance=.)} %>% 
  write.csv("Outputs/Other/Var-Imp_UMS0.1.1.csv", row.names=FALSE)

##UMS 0.2
readRDS("Outputs/Diagnostic-Files/Temp-Autocoders/Run-UMS_UMS0.2.Rds") %>% 
  map_dfr(~ .x %>% 
            pluck("finalModel", "variable.importance") %>% 
            {tibble(Measure=names(.), value=.)},
          .id="name") %>% 
  pivot_wider(names_prefix="Importance_") %>% 
  write.csv("Outputs/Other/Var-Imp_UMS0.2.csv", row.names=FALSE)
```

- This step isn't strictly necessary for the R code in this document, but it allows us to run [`umsData()`](index#r-scripts) for all UMSs without needing to be on the computer where the auto-coders are saved (if it's different).

### Analyze fairness and performance

- We'll start reading fairness/performance data for the round 1 auto-coders, and add baseline data:

```{r, message=FALSE}
##Read
perf_baseline <- read_csv("Outputs/Performance/Perf_Baseline.csv")
perf_round1 <- rbind(perf_baseline,
                     read_csv("Outputs/Performance/Perf_UMS-Round1.csv"))

##Decode first digit of UMS code
categories <- c("Baseline", "Downsampling", "Valid pred selection", 
                "Normalization", "Combination") %>% 
  set_names(0:4)

##Shape performance dataframe for plotting: Fairness/Performance in separate
##  columns, one row per UMS * metric, Category factor, shorter Classifier label
perfPlot_round1 <- perf_round1 %>%
  select(Classifier,
         Acc, Acc_Diff, matches("ClassAcc_(Present|Absent)$"),
         matches("ClassAcc_(Present|Absent)_Diff")) %>%
  ##Fairness/Performance in separate columns, one row per UMS * metric
  pivot_longer(contains("Acc")) %>%
  mutate(Metric = fct_inorder(if_else(str_detect(name, "Absent|Present"),
                                      paste(str_extract(name, "Absent|Present"), "class accuracy"),
                                      "Overall accuracy")),
         name = if_else(str_detect(name, "Diff"), "Fairness", "Performance")) %>%
  pivot_wider() %>%
  ##Add Category column, shorter Classifier label
  mutate(Category = recode_factor(str_extract(Classifier, "\\d"), !!!categories),
         across(Classifier, ~ str_remove(.x, "UMS")))
```


```{r}
##Plot
perfPlot_round1 %>% 
  mutate(across(Fairness, abs)) %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=20) +
  ##Each metric in its own facet
  ##  (N.B. use arg scales="free" to have each facet zoom to fit data)
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw()
```

- To make the baseline stand out, we can plot it with separate aesthetics:

```{r}
perfPlot_round1 %>%
  mutate(across(Fairness, abs)) %>% 
  ##Exclude Baseline from points & labels
  filter(Classifier != "0.0") %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=20) +
  ##Dotted line for baseline
  geom_vline(data=perfPlot_round1 %>% filter(Classifier=="0.0"), 
             aes(xintercept=abs(Fairness)), linetype="dashed") +
  geom_hline(data=perfPlot_round1 %>% filter(Classifier=="0.0"), 
             aes(yintercept=Performance), linetype="dashed") +
  ##Each metric in its own facet
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw()
```

- There are several UMSs that  improve on fairness relative to the baseline, but there isn't a clear cut optimal UMS, i.e., no UMS does well across all metrics. UMS 1.3.1. is better in overall fairness and accuracy and present class fairness and accuracy, but both performance and fairness are poor for absent class accuracy. UMS 1.2 is better in overall fairness and around the same in performance as the baseline in overall accuracy, better for both metrics in absent class accuracy, but worse in both fairness and performance in present class accuracy (although not as markedly). The remaining metrics cluster around the baseline.

## UMS round 2

- Given the mixed results of the first round of UMS, we'll try some combination strategies: combining downsampling with valid predictor selection. We attempted a total of eight combinations.

### Run auto-coders and extract metrics {#ums2-run}

- We'll load Bash, navigate to `Shell-Scripts/`, and run the following. The results will be saved in the same directory.

```{bash, eval=FALSE}
##Run directly
bash UMS-Round2.sh &> ../Outputs/Shell-Scripts/UMS-Round2.out
```

- To extract and save fairness/performance metrics from the round 2 auto-coders, we switch back to R and run the following code:

```{r, eval=params$extract_metrics}
##Get auto-coder filenames (all combo UMSs start with the digit 4)
files_round2 <- list.files("Outputs/Diagnostic-Files/Temp-Autocoders/", 
                           "Run-UMS_UMS4", full.names=TRUE)

##Read auto-coder files
cls_round2 <-
  files_round2 %>% 
  ##Better names
  set_names(str_remove_all(files_round2, ".+_|\\.Rds")) %>% 
  map(readRDS)

##Extract performance
cls_round2 %>% 
  map_dfr(cls_summary, .id="Classifier") %>% 
  ##Add long description
  left_join(umsList %>% mutate(Classifier = paste0("UMS", UMS)) %>% select(-UMS),
            by="Classifier") %>% 
  ##Save data
  write_csv("Outputs/Performance/Perf_UMS-Round2.csv")
```


### Analyze fairness and performance

- Read fairness/performance data for the round 1 auto-coders, and add baseline data:

```{r, message=FALSE}
##Read
perf_baseline <- read_csv("Outputs/Performance/Perf_Baseline.csv")
perf_round2 <- rbind(perf_baseline,
                     read_csv("Outputs/Performance/Perf_UMS-Round2.csv"))

##Shape performance dataframe for plotting: Fairness/Performance in separate
##  columns, one row per UMS * metric, Category factor, shorter Classifier label
perfPlot_round2 <- perf_round2 %>%
  select(Classifier,
         Acc, Acc_Diff, matches("ClassAcc_(Present|Absent)$"),
         matches("ClassAcc_(Present|Absent)_Diff")) %>%
  ##Fairness/Performance in separate columns, one row per UMS * metric
  pivot_longer(contains("Acc")) %>%
  mutate(Metric = fct_inorder(if_else(str_detect(name, "Absent|Present"),
                                      paste(str_extract(name, "Absent|Present"), "class accuracy"),
                                      "Overall accuracy")),
         name = if_else(str_detect(name, "Diff"), "Fairness", "Performance")) %>%
  pivot_wider() %>%
  ##Add Category column, shorter Classifier label
  mutate(Category = recode_factor(str_extract(Classifier, "\\d"), !!!categories),
         across(Classifier, ~ str_remove(.x, "UMS")))
```

- Plot (using dotted line for baseline)

```{r}
perfPlot_round2 %>%
  mutate(across(Fairness, abs)) %>% 
  ##Exclude Baseline from points & labels
  filter(Classifier != "0.0") %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=20, color="black") +
  ##Dotted line for baseline
  geom_vline(data=perfPlot_round2 %>% filter(Classifier=="0.0"), 
             aes(xintercept=abs(Fairness)), linetype="dashed") +
  geom_hline(data=perfPlot_round2 %>% filter(Classifier=="0.0"), 
             aes(yintercept=Performance), linetype="dashed") +
  ##Each metric in its own facet
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw() +
  ##Rotate x-axis labels to avoid clash
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```


- We can also plot round 1 & 2 together:

```{r}
##Put perfPlot dfs together
perfPlot <- rbind(perfPlot_round1,
                  perfPlot_round2) %>% 
  ##Remove duplicate Baseline rows
  distinct()

perfPlot %>%
  mutate(across(Fairness, abs)) %>% 
  ##Exclude Baseline from points
  filter(Classifier != "0.0") %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=40) +
  ##Dotted line for baseline
  geom_vline(data=perfPlot %>% filter(Classifier=="0.0"), 
             aes(xintercept=abs(Fairness)), linetype="dashed") +
  geom_hline(data=perfPlot %>% filter(Classifier=="0.0"), 
             aes(yintercept=Performance), linetype="dashed") +
  ##Each metric in its own facet
  facet_wrap(~ Metric, nrow=1) +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw() +
  ##Rotate x-axis labels to avoid clash
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

- Upon a visual examination the combinations overall appear to do a better job overall than the UMSs by the themselves. However, some combinations (e.g., 4.2.1, 4.3.1, 4.1., 4.4.3) appear to mimic the massive effect of downsampling strategies that makes them perform poorly and efficiently with only one of two metrics. 

### Identify optimal auto-coder

- The final step is to identify an optimal autoc-coder. At present, although there are clusters of better or worse UMSs, there isn’t a single UMS that clearly stands out from the rest. To wind down options we will find the UMS (or combination thereof) that is Pareto-optimal (i.e., the one good fairness–performance tradeoff, see another example with a gender classifier [here](https://djvill.github.io/SLAC-Fairness/Analysis-Walkthrough)). In R, we can use `psel()` from the rPref package to find Pareto-optimal auto-coders. For example, the following auto-coders are Pareto-optimal for Overall accuracy:

```{r}
perfPlot %>% 
  mutate(across(Fairness, abs)) %>% 
  filter(Metric=="Overall accuracy") %>% 
  psel(high(Performance) * low(Fairness))
```
In a plot:

```{r}
perfPlot %>% 
  mutate(across(Fairness, abs)) %>% 
  filter(Metric=="Overall accuracy",
         Classifier != "0.0") %>%
  mutate(`Pareto-optimal` = Classifier %in% psel(., high(Performance) * low(Fairness))$Classifier) %>% 
  ggplot(aes(x=Fairness, y=Performance, color=Category, label=Classifier, alpha=`Pareto-optimal`)) +
  ##Points w/ ggrepel'd labels
  geom_point(size=3) +
  geom_text_repel(show.legend=FALSE, max.overlaps=40) +
  ##Dotted line for baseline
  geom_vline(data=perfPlot %>% 
               filter(Metric=="Overall accuracy", Classifier=="0.0"), 
             aes(xintercept=abs(Fairness)), linetype="dashed") +
  geom_hline(data=perfPlot %>% 
               filter(Metric=="Overall accuracy", Classifier=="0.0"), 
             aes(yintercept=Performance), linetype="dashed") +
  ##Lower fairness on the right (so top-right is optimal)
  scale_x_reverse() +
  ##Theme
  theme_bw() +
  ##Rotate x-axis labels to avoid clash
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```
- We have 3 metrics (overall accuracy, present class accuracy, and absent class accuracy), so for each metric we find the UMSs that are Pareto-optimal for the fairness–performance tradeoff:

```{r}
##Get list of dataframes, one per Metric, w/ only Pareto-optimal UMSs
paretoOpt <- 
  perfPlot %>% 
  mutate(across(Fairness, abs)) %>% 
  group_by(Metric) %>% 
  ##Identify Pareto-optimal UMSs
  group_map(~ psel(.x, high(Performance) * low(Fairness)),
            .keep=TRUE)
##Display as a single dataframe
bind_rows(paretoOpt)
```

- There is one UMS (4.4.3) that is Pareto-optimal for two metrics. The remaining UMSs are only pareto-optimal for a single metric.  As seen below, however, no UMS is Pareto-optimal for all three metrics:

```{r}
##Get Classifier value that is in all 3 dataframes (if any)
paretoOpt %>% 
  map("Classifier") %>% 
  reduce(intersect)
```
- When we isolate metrics, 4.4.3 also happens to be the fairest UMS for Present class accuracy. It's worth noting that Present is the larger class (approximately 2/3 of the tokens). However, it does poorly in both fairness and performance for the Absent class.

```{r}
##Get fairest UMS for each Metric
perfPlot %>% 
  mutate(across(Fairness, abs)) %>% 
  group_by(Metric) %>%
  filter(Fairness==min(Fairness))
```

# Script meta-info {#script-info}

## R session info

```{r, eval=TRUE}
sessionInfo()
```


## Disk space used

These are only shown if `params$extract_metrics` is `TRUE` (because otherwise it's assumed that you're not working on the same system the auto-coders were run on).

Temporary auto-coders:

```{r, eval=params$extract_metrics}
tmpAuto <- 
  list.files("Outputs/Diagnostic-Files/Temp-Autocoders/", 
             # "^(Run-UMS|(Hyperparam-Tuning|Outlier-Dropping)_UMS0\\.0).*Rds$") %>%
             "^Run-UMS.*Rds$", full.names=TRUE) %>%
  file.info()
# cat("Disk space: ", round(sum(tmpAuto$size)/2^30, 1), " Gb (",
cat("Disk space: ", round(sum(tmpAuto$size)/2^20, 1), " Mb (",
    nrow(tmpAuto), " files)", sep="")
```

Complete repository:

```{r, eval=params$extract_metrics}
if (.Platform$OS.type=="windows") {
  shell("dir /s", intern=TRUE) %>% 
    tail(2) %>% 
    head(1) %>% 
    str_trim() %>% 
    str_squish()
}
if (.Platform$OS.type=="unix") {
  system2("du", "-sh", stdout=TRUE) %>% 
    str_remove("\\s.+") %>% 
    paste0("b")
}
```

## Machine specs

System:

```{r, eval=TRUE}
Sys.info()
```

Processor:

```{r, eval=TRUE}
get_cpu()
```

RAM:

```{r, eval=TRUE}
#get_ram()
```


## Running time

Total running time for R code in this document (with `params$extract_metrics` set to `r params$extract_metrics`), in seconds:

```{r, eval=TRUE}
timing$stop <- proc.time()

timing$stop - timing$start
```

<!-- Footer: Add CSS styling -->

```{css, echo=FALSE}
/* Add scrollbars to long R input/output blocks */
pre {
  max-height: 300px;
  overflow-y: auto;
}

/* Repo navigation block */
div#repo-nav {
  background-color: #ddd;
  width: fit-content;
  padding: 10px 20px;
  border: 1px solid #bbb;
  border-radius: 20px;
}
div#repo-nav * {
  margin: 0px;
  font-style: italic;
}
```

